{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08e09d48",
   "metadata": {},
   "source": [
    "# BERT based NER experiment\n",
    "> Tutorial author: 徐欣（<xxucs@zju.edu.cn>）\n",
    "\n",
    "On this demo, we use `BERT` to recognize named entities. We hope this demo can help you understand the process of named entity recognition.\n",
    "\n",
    "This demo uses `Python3`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cc3a6d",
   "metadata": {},
   "source": [
    "## NER\n",
    "**Named-entity recognition** (also known as named entity identification, entity chunking, and entity extraction) is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13b1128",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "In this demo, we use dataset [**CoNLL-2003**](https://www.clips.uantwerpen.be/conll2003/ner/). It is a dataset for NER, concentrating on four types of named entities related to persons, locations, organizations, and names of miscellaneous entities.\n",
    "\n",
    "|    Word    | Part-of-speech (POS) tag | Syntactic chunk tag | Named entity tag |\n",
    "| :--------: | :----------------------: | :-----------------: | :--------------: |\n",
    "|  Pakistan  |           NNP            |        B-NP         |      B-LOC       |\n",
    "|     ,      |            ,             |          O          |        O         |\n",
    "|    who     |            WP            |        B-NP         |        O         |\n",
    "|   arrive   |           VBP            |        B-VP         |        O         |\n",
    "|     in     |            IN            |        B-PP         |        O         |\n",
    "| Australia  |           NNP            |        B-NP         |      B-LOC       |\n",
    "|   later    |            JJ            |        B-NP         |        O         |\n",
    "|    this    |            DT            |        I-NP         |        O         |\n",
    "|   month    |            NN            |        I-NP         |        O         |\n",
    "|     ,      |            ,             |          O          |        O         |\n",
    "|    are     |           VBP            |        B-VP         |        O         |\n",
    "|    the     |            DT            |        B-NP         |        O         |\n",
    "|   other    |            JJ            |        I-NP         |        O         |\n",
    "|    team    |            NN            |        I-NP         |        O         |\n",
    "| competing  |           VBG            |        B-VP         |        O         |\n",
    "|     in     |            IN            |        B-PP         |        O         |\n",
    "|    the     |            DT            |        B-NP         |        O         |\n",
    "|   World    |           NNP            |        I-NP         |      B-MISC      |\n",
    "|   Series   |           NNP            |        I-NP         |      I-MISC      |\n",
    "| tournament |            NN            |        I-NP         |        O         |\n",
    "|     .      |            .             |          O          |        O         |\n",
    "\n",
    "- train.txt: It contains 14,987 sentences\n",
    "- valid.txt: It contains 3,466 sentences\n",
    "- test.txt: It contains 3,684 sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47715483",
   "metadata": {},
   "source": [
    "## BERT\n",
    "[**Bidirectional Encoder Representations from Transformers (BERT)**](https://github.com/google-research/bert) is a transformer-based machine learning technique for natural language processing (NLP) pre-training developed by Google."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b0cf3f",
   "metadata": {},
   "source": [
    "## Prepare the runtime environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd23f8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch-transformers==1.2.0\n",
    "!pip install torch==1.2.0\n",
    "!pip install seqeval==0.0.5\n",
    "!pip install tqdm==4.31.1\n",
    "!pip install nltk==3.4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a346b5",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33f17d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import argparse\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_transformers import (WEIGHTS_NAME, AdamW, BertConfig, BertForTokenClassification, BertTokenizer, WarmupLinearSchedule)\n",
    "from torch import nn\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de20573b",
   "metadata": {},
   "source": [
    "## Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ff2c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(filename):\n",
    "    '''\n",
    "    read file\n",
    "    '''\n",
    "    f = open(filename)\n",
    "    data = []\n",
    "    sentence = []\n",
    "    label= []\n",
    "    for line in f:\n",
    "        if len(line)==0 or line.startswith('-DOCSTART') or line[0]==\"\\n\":\n",
    "            if len(sentence) > 0:\n",
    "                data.append((sentence,label))\n",
    "                sentence = []\n",
    "                label = []\n",
    "            continue\n",
    "        splits = line.split(' ')\n",
    "        sentence.append(splits[0])\n",
    "        label.append(splits[-1][:-1])\n",
    "\n",
    "    if len(sentence) >0:\n",
    "        data.append((sentence,label))\n",
    "        sentence = []\n",
    "        label = []\n",
    "    return data\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id, valid_ids=None, label_mask=None):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        self.valid_ids = valid_ids\n",
    "        self.label_mask = label_mask\n",
    "\n",
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        return readfile(input_file)\n",
    "\n",
    "\n",
    "class NerProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the CoNLL-2003 data set.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"train.txt\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"valid.txt\")), \"dev\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"test.txt\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        return [\"O\", \"B-MISC\", \"I-MISC\",  \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"[CLS]\", \"[SEP]\"]\n",
    "\n",
    "    def _create_examples(self,lines,set_type):\n",
    "        examples = []\n",
    "        for i,(sentence,label) in enumerate(lines):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = ' '.join(sentence)\n",
    "            text_b = None\n",
    "            label = label\n",
    "            examples.append(InputExample(guid=guid,text_a=text_a,text_b=text_b,label=label))\n",
    "        return examples\n",
    "\n",
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    label_map = {label : i for i, label in enumerate(label_list,1)}\n",
    "\n",
    "    features = []\n",
    "    for (ex_index,example) in enumerate(examples):\n",
    "        textlist = example.text_a.split(' ')\n",
    "        labellist = example.label\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        valid = []\n",
    "        label_mask = []\n",
    "        for i, word in enumerate(textlist):\n",
    "            token = tokenizer.tokenize(word)\n",
    "            tokens.extend(token)\n",
    "            label_1 = labellist[i]\n",
    "            for m in range(len(token)):\n",
    "                if m == 0:\n",
    "                    labels.append(label_1)\n",
    "                    valid.append(1)\n",
    "                    label_mask.append(1)\n",
    "                else:\n",
    "                    valid.append(0)\n",
    "        if len(tokens) >= max_seq_length - 1:\n",
    "            tokens = tokens[0:(max_seq_length - 2)]\n",
    "            labels = labels[0:(max_seq_length - 2)]\n",
    "            valid = valid[0:(max_seq_length - 2)]\n",
    "            label_mask = label_mask[0:(max_seq_length - 2)]\n",
    "        ntokens = []\n",
    "        segment_ids = []\n",
    "        label_ids = []\n",
    "        ntokens.append(\"[CLS]\")\n",
    "        segment_ids.append(0)\n",
    "        valid.insert(0,1)\n",
    "        label_mask.insert(0,1)\n",
    "        label_ids.append(label_map[\"[CLS]\"])\n",
    "        for i, token in enumerate(tokens):\n",
    "            ntokens.append(token)\n",
    "            segment_ids.append(0)\n",
    "            if len(labels) > i:\n",
    "                label_ids.append(label_map[labels[i]])\n",
    "        ntokens.append(\"[SEP]\")\n",
    "        segment_ids.append(0)\n",
    "        valid.append(1)\n",
    "        label_mask.append(1)\n",
    "        label_ids.append(label_map[\"[SEP]\"])\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(ntokens)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        label_mask = [1] * len(label_ids)\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "            label_ids.append(0)\n",
    "            valid.append(1)\n",
    "            label_mask.append(0)\n",
    "        while len(label_ids) < max_seq_length:\n",
    "            label_ids.append(0)\n",
    "            label_mask.append(0)\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        assert len(label_ids) == max_seq_length\n",
    "        assert len(valid) == max_seq_length\n",
    "        assert len(label_mask) == max_seq_length\n",
    "\n",
    "        if ex_index < 5:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\" % (example.guid))\n",
    "            logger.info(\"tokens: %s\" % \" \".join(\n",
    "                    [str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            # logger.info(\"label: %s (id = %d)\" % (example.label, label_ids))\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_id=label_ids,\n",
    "                              valid_ids=valid,\n",
    "                              label_mask=label_mask))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aedb530",
   "metadata": {},
   "source": [
    "## BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4abd61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ner(BertForTokenClassification):\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None,valid_ids=None,attention_mask_label=None):\n",
    "        sequence_output = self.bert(input_ids, token_type_ids, attention_mask,head_mask=None)[0]\n",
    "        batch_size,max_len,feat_dim = sequence_output.shape\n",
    "        valid_output = torch.zeros(batch_size,max_len,feat_dim,dtype=torch.float32,device='cuda')\n",
    "        for i in range(batch_size):\n",
    "            jj = -1\n",
    "            for j in range(max_len):\n",
    "                    if valid_ids[i][j].item() == 1:\n",
    "                        jj += 1\n",
    "                        valid_output[i][jj] = sequence_output[i][j]\n",
    "        sequence_output = self.dropout(valid_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=0)\n",
    "            # Only keep active parts of the loss\n",
    "            # attention_mask_label = None\n",
    "            if attention_mask_label is not None:\n",
    "                active_loss = attention_mask_label.view(-1) == 1\n",
    "                active_logits = logits.view(-1, self.num_labels)[active_loss]\n",
    "                active_labels = labels.view(-1)[active_loss]\n",
    "                loss = loss_fct(active_logits, active_labels)\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            return loss\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029e661b",
   "metadata": {},
   "source": [
    "## Fine-Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae701820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required parameters\n",
    "data_dir = \"data/\"\n",
    "bert_model = \"bert-base-cased\"\n",
    "task_name = \"ner\"\n",
    "output_dir = \"out_ner\"\n",
    "max_seq_length = 128\n",
    "do_train = True\n",
    "do_eval = True\n",
    "eval_on = \"dev\"\n",
    "do_lower_case = \"True\"\n",
    "train_batch_size = 32\n",
    "eval_batch_size = 8\n",
    "learning_rate = 5e-5\n",
    "num_train_epochs = 5.0         # the number of training epochs\n",
    "warmup_proportion = 0.1\n",
    "weight_decay = 0.01\n",
    "adam_epsilon = 1e-8\n",
    "max_grad_norm = 1.0\n",
    "no_cuda = False\n",
    "local_rank = -1\n",
    "seed = 42\n",
    "gradient_accumulation_steps = 1\n",
    "fp16 = False\n",
    "fp16_opt_level = \"01\"\n",
    "loss_scale = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4ced0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "processors = {\"ner\":NerProcessor}\n",
    "\n",
    "if local_rank ==-1 or no_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not no_cuda else \"cpu\")\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "else:\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    device = torch.device(\"cuda\", local_rank)\n",
    "    n_gpu = 1\n",
    "    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.distributed.init_process_group(backend='nccl')\n",
    "\n",
    "logger.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(device, n_gpu, bool(local_rank != -1), fp16))\n",
    "\n",
    "train_batch_size = train_batch_size // gradient_accumulation_steps\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if os.path.exists(output_dir) and os.listdir(output_dir) and do_train:\n",
    "    raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(output_dir))\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "task_name = task_name.lower()\n",
    "\n",
    "processor = processors[task_name]()\n",
    "label_list = processor.get_labels()\n",
    "num_labels = len(label_list) + 1\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=do_lower_case)\n",
    "\n",
    "train_examples = None\n",
    "num_train_optimization_steps = 0\n",
    "if do_train:\n",
    "    train_examples = processor.get_train_examples(data_dir)\n",
    "    num_train_optimization_steps = int(len(train_examples) / train_batch_size / gradient_accumulation_steps) * num_train_epochs\n",
    "    if local_rank != -1:\n",
    "        num_train_optimization_steps = num_train_optimization_steps // torch.distributed.get_world_size()\n",
    "\n",
    "if local_rank not in [-1, 0]:\n",
    "    torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43846012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model\n",
    "config = BertConfig.from_pretrained(bert_model, num_labels=num_labels, finetuning_task=task_name)\n",
    "model = Ner.from_pretrained(bert_model, from_tf = False, config = config)\n",
    "\n",
    "if local_rank == 0:\n",
    "    torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias','LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "warmup_steps = int(warmup_proportion * num_train_optimization_steps)\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
    "scheduler = WarmupLinearSchedule(optimizer, warmup_steps=warmup_steps, t_total=num_train_optimization_steps)\n",
    "if fp16:\n",
    "    try:\n",
    "        from apex import amp\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "    model, optimizer = amp.initialize(model, optimizer, opt_level=fp16_opt_level)\n",
    "\n",
    "# multi-gpu training (should be after apex fp16 initialization)\n",
    "if n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "if local_rank != -1:\n",
    "    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank],\n",
    "                                                      output_device=local_rank,\n",
    "                                                      find_unused_parameters=True)\n",
    "global_step = 0\n",
    "nb_tr_steps = 0\n",
    "tr_loss = 0\n",
    "label_map = {i : label for i, label in enumerate(label_list,1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd6d3c3",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e71425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "if do_train:\n",
    "    train_features = convert_examples_to_features(\n",
    "        train_examples, label_list, max_seq_length, tokenizer)\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "    logger.info(\"  Batch size = %d\", train_batch_size)\n",
    "    logger.info(\"  Num steps = %d\", num_train_optimization_steps)\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "    all_valid_ids = torch.tensor([f.valid_ids for f in train_features], dtype=torch.long)\n",
    "    all_lmask_ids = torch.tensor([f.label_mask for f in train_features], dtype=torch.long)\n",
    "    train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids,all_valid_ids,all_lmask_ids)\n",
    "    if local_rank == -1:\n",
    "        train_sampler = RandomSampler(train_data)\n",
    "    else:\n",
    "        train_sampler = DistributedSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\n",
    "\n",
    "    model.train()\n",
    "    for _ in trange(int(num_train_epochs), desc=\"Epoch\"):\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, label_ids, valid_ids,l_mask = batch\n",
    "            loss = model(input_ids, segment_ids, input_mask, label_ids,valid_ids,l_mask)\n",
    "            if n_gpu > 1:\n",
    "                loss = loss.mean() # mean() to average on multi-gpu.\n",
    "            if gradient_accumulation_steps > 1:\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "\n",
    "            if fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), max_grad_norm)\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "    # Save a trained model and the associated configuration\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "    model_to_save.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    label_map = {i : label for i, label in enumerate(label_list,1)}\n",
    "    model_config = {\"bert_model\":bert_model,\"do_lower\":do_lower_case,\"max_seq_length\":max_seq_length,\"num_labels\":len(label_list)+1,\"label_map\":label_map}\n",
    "    json.dump(model_config,open(os.path.join(output_dir,\"model_config.json\"),\"w\"))\n",
    "    # Load a trained model and config that you have fine-tuned\n",
    "else:\n",
    "    # Load a trained model and vocabulary that you have fine-tuned\n",
    "    model = Ner.from_pretrained(output_dir)\n",
    "    tokenizer = BertTokenizer.from_pretrained(output_dir, do_lower_case=do_lower_case)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5853c4",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fecf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "if do_eval and (local_rank == -1 or torch.distributed.get_rank() == 0):\n",
    "    if eval_on == \"dev\":\n",
    "        eval_examples = processor.get_dev_examples(data_dir)\n",
    "    elif eval_on == \"test\":\n",
    "        eval_examples = processor.get_test_examples(data_dir)\n",
    "    else:\n",
    "        raise ValueError(\"eval on dev or test set only\")\n",
    "    eval_features = convert_examples_to_features(eval_examples, label_list, max_seq_length, tokenizer)\n",
    "    logger.info(\"***** Running evaluation *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "    logger.info(\"  Batch size = %d\", eval_batch_size)\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "    all_valid_ids = torch.tensor([f.valid_ids for f in eval_features], dtype=torch.long)\n",
    "    all_lmask_ids = torch.tensor([f.label_mask for f in eval_features], dtype=torch.long)\n",
    "    eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids,all_valid_ids,all_lmask_ids)\n",
    "    # Run prediction for full data\n",
    "    eval_sampler = SequentialSampler(eval_data)\n",
    "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    label_map = {i : label for i, label in enumerate(label_list,1)}\n",
    "    for input_ids, input_mask, segment_ids, label_ids,valid_ids,l_mask in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "        valid_ids = valid_ids.to(device)\n",
    "        label_ids = label_ids.to(device)\n",
    "        l_mask = l_mask.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids, segment_ids, input_mask,valid_ids=valid_ids,attention_mask_label=l_mask)\n",
    "\n",
    "        logits = torch.argmax(F.log_softmax(logits,dim=2),dim=2)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = label_ids.to('cpu').numpy()\n",
    "        input_mask = input_mask.to('cpu').numpy()\n",
    "\n",
    "        for i, label in enumerate(label_ids):\n",
    "            temp_1 = []\n",
    "            temp_2 = []\n",
    "            for j,m in enumerate(label):\n",
    "                if j == 0:\n",
    "                    continue\n",
    "                elif label_ids[i][j] == len(label_map):\n",
    "                    y_true.append(temp_1)\n",
    "                    y_pred.append(temp_2)\n",
    "                    break\n",
    "                else:\n",
    "                    temp_1.append(label_map[label_ids[i][j]])\n",
    "                    temp_2.append(label_map[logits[i][j]])\n",
    "\n",
    "    report = classification_report(y_true, y_pred,digits=4)\n",
    "    logger.info(\"\\n%s\", report)\n",
    "    output_eval_file = os.path.join(output_dir, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        logger.info(\"***** Eval results *****\")\n",
    "        logger.info(\"\\n%s\", report)\n",
    "        writer.write(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0f79a8",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78c5102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make changes for better representation and display of 'entity detected' and their 'entity types' for the given sentence to test or inference\n",
    "from nltk import word_tokenize\n",
    "from collections import OrderedDict\n",
    "\n",
    "class BertNer(BertForTokenClassification):\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, valid_ids=None):\n",
    "        sequence_output = self.bert(input_ids, token_type_ids, attention_mask, head_mask=None)[0]\n",
    "        batch_size,max_len,feat_dim = sequence_output.shape\n",
    "        valid_output = torch.zeros(batch_size,max_len,feat_dim,dtype=torch.float32,device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        for i in range(batch_size):\n",
    "            jj = -1\n",
    "            for j in range(max_len):\n",
    "                    if valid_ids[i][j].item() == 1:\n",
    "                        jj += 1\n",
    "                        valid_output[i][jj] = sequence_output[i][j]\n",
    "        sequence_output = self.dropout(valid_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        return logits\n",
    "\n",
    "class Ner:\n",
    "\n",
    "    def __init__(self,model_dir: str):\n",
    "        self.model , self.tokenizer, self.model_config = self.load_model(model_dir)\n",
    "        self.label_map = self.model_config[\"label_map\"]\n",
    "        self.max_seq_length = self.model_config[\"max_seq_length\"]\n",
    "        self.label_map = {int(k):v for k,v in self.label_map.items()}\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def load_model(self, model_dir: str, model_config: str = \"model_config.json\"):\n",
    "        model_config = os.path.join(model_dir,model_config)\n",
    "        model_config = json.load(open(model_config))\n",
    "        model = BertNer.from_pretrained(model_dir)\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_dir, do_lower_case=model_config[\"do_lower\"])\n",
    "        return model, tokenizer, model_config\n",
    "\n",
    "    def tokenize(self, text: str):\n",
    "        \"\"\" tokenize input\"\"\"\n",
    "        words = word_tokenize(text)\n",
    "        tokens = []\n",
    "        valid_positions = []\n",
    "        for i,word in enumerate(words):\n",
    "            token = self.tokenizer.tokenize(word)\n",
    "            tokens.extend(token)\n",
    "            for i in range(len(token)):\n",
    "                if i == 0:\n",
    "                    valid_positions.append(1)\n",
    "                else:\n",
    "                    valid_positions.append(0)\n",
    "        return tokens, valid_positions\n",
    "\n",
    "    def preprocess(self, text: str):\n",
    "        \"\"\" preprocess \"\"\"\n",
    "        tokens, valid_positions = self.tokenize(text)\n",
    "        ## insert \"[CLS]\"\n",
    "        tokens.insert(0,\"[CLS]\")\n",
    "        valid_positions.insert(0,1)\n",
    "        ## insert \"[SEP]\"\n",
    "        tokens.append(\"[SEP]\")\n",
    "        valid_positions.append(1)\n",
    "        segment_ids = []\n",
    "        for i in range(len(tokens)):\n",
    "            segment_ids.append(0)\n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        while len(input_ids) < self.max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "            valid_positions.append(0)\n",
    "        return input_ids,input_mask,segment_ids,valid_positions\n",
    "\n",
    "    def predict(self, text: str):\n",
    "        input_ids,input_mask,segment_ids,valid_ids = self.preprocess(text)\n",
    "        input_ids = torch.tensor([input_ids],dtype=torch.long,device=self.device)\n",
    "        input_mask = torch.tensor([input_mask],dtype=torch.long,device=self.device)\n",
    "        segment_ids = torch.tensor([segment_ids],dtype=torch.long,device=self.device)\n",
    "        valid_ids = torch.tensor([valid_ids],dtype=torch.long,device=self.device)\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(input_ids, segment_ids, input_mask,valid_ids)\n",
    "        logits = F.softmax(logits,dim=2)\n",
    "        logits_label = torch.argmax(logits,dim=2)\n",
    "        logits_label = logits_label.detach().cpu().numpy().tolist()[0]\n",
    "\n",
    "        logits_confidence = [values[label].item() for values,label in zip(logits[0],logits_label)]\n",
    "\n",
    "        logits = []\n",
    "        pos = 0\n",
    "        for index,mask in enumerate(valid_ids[0]):\n",
    "            if index == 0:\n",
    "                continue\n",
    "            if mask == 1:\n",
    "                logits.append((logits_label[index-pos],logits_confidence[index-pos]))\n",
    "            else:\n",
    "                pos += 1\n",
    "        logits.pop()\n",
    "\n",
    "        labels = [(self.label_map[label],confidence) for label,confidence in logits]\n",
    "        words = word_tokenize(text)\n",
    "        assert len(labels) == len(words)\n",
    "\n",
    "        result = []\n",
    "        for word, (label, confidence) in zip(words, labels):\n",
    "            if label!='O':\n",
    "                result.append((word,label))\n",
    "        tmp = []\n",
    "        tag = OrderedDict()\n",
    "        tag['PER'] = []\n",
    "        tag['LOC'] = []\n",
    "        tag['ORG'] = []\n",
    "        tag['MISC'] = []\n",
    "        \n",
    "        for i, (word, label) in enumerate(result):\n",
    "            if label=='B-PER' or label=='B-LOC' or label=='B-ORG' or label=='B-MISC':\n",
    "                if i==0:\n",
    "                    tmp.append(word)\n",
    "                else:\n",
    "                    wordstype = result[i-1][1][2:]\n",
    "                    tag[wordstype].append(' '.join(tmp))\n",
    "                    tmp.clear()\n",
    "                    tmp.append(word)\n",
    "            elif i==len(result)-1:\n",
    "                tmp.append(word)\n",
    "                wordstype = result[i][1][2:]\n",
    "                tag[wordstype].append(' '.join(tmp))\n",
    "            else:\n",
    "                tmp.append(word)\n",
    "\n",
    "        return tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fec9c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run below command for import and download 'nltk' library as it is important for predictions of entities of the sentence\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# If it's too slow to download 'nltk_data', we offer it in 'data/nltk_data' and you can use it by running the following code.\n",
    "# import nltk\n",
    "# nltk.data.path.insert(0,'./data/nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce89e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ner(\"out_ner/\")\n",
    "\n",
    "# Text to be NERed\n",
    "text = \"Irene, a master student in Zhejiang University, Hangzhou, is traveling in Warsaw for Chopin Music Festival.\"\n",
    "\n",
    "print(\"The text to be NERed:\")\n",
    "print(text)\n",
    "print('Results of NER:')\n",
    "\n",
    "result = model.predict(text)\n",
    "for k,v in result.items():\n",
    "    if v:\n",
    "        print(v,end=': ')\n",
    "        if k=='PER':\n",
    "            print('Person')\n",
    "        elif k=='LOC':\n",
    "            print('Location')\n",
    "        elif k=='ORG':\n",
    "            print('Organization')\n",
    "        elif k=='MISC':\n",
    "            print('Miscellaneous')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fc8159",
   "metadata": {},
   "source": [
    "This demo does not include parameter adjustment. If you can interested in this, you can go to [deepke](http://openkg.cn/tool/deepke)\n",
    "Warehouse, download and use more models:)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
