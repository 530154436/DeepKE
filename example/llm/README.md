<p align="left">
    <b> English | <a href="https://github.com/zjunlp/DeepKE/blob/llm/example/llm/README_CN.md">简体中文</a> </b>
</p>

- [CaMA](#cama)
  - [Training](#training)
  - [Usage](#usage)
- [LLaMA](#llama)
  - [CodeKGC-Code Language Models for Knowledge Graph Construction English | Chinese](#codekgc-code-language-models-for-knowledge-graph-construction-english--chinese)
- [ChatGLM](#chatglm)
- [GPT series](#gpt-series)
  - [Instruct Large Language Models(LLMs) using In-Context Learning](#instruct-large-language-modelsllms-using-in-context-learning)
    - [Information Extraction with LLMs English | Chinese](#information-extraction-with-llms-english--chinese)
    - [Data Augmentation with LLMs English | Chinese](#data-augmentation-with-llms-english--chinese)
    - [CCKS2023 Instruction-based Knowledge Graph Construction with LLMs English | Chinese](#ccks2023-instruction-based-knowledge-graph-construction-with-llms-english--chinese)

---

## CaMA

<p align="center" width="100%">
<a href="" target="_blank"><img src="assets/cama_logo.jpeg" alt="ZJU-CaMA" style="width: 20%; min-width: 20px; display: block; margin: auto;"></a>
</p>

### Training

### Usage
 
---

## LLaMA

<p align="center" width="100%">
<a href="" target="_blank"><img src="assets/llama_logo.jpeg" alt="LLaMA" style="width: 20%; min-width: 20px; display: block; margin: auto;"></a>
</p>

### CodeKGC-Code Language Models for Knowledge Graph Construction [English](./CodeKGC/README.md) | [Chinese](./CodeKGC/README_CN.md)

To better address Relational Triple Extraction (rte) task in Knowledge Graph Construction, we have designed code-style prompts to model the structure of  Relational Triple, and used Code-LLMs to generate more accurate predictions. The key step of code-style prompt construction is to transform (text, output triples) pairs into semantically equivalent program language written in Python.

<div align=center>
<img src="./CodeKGC/codekgc_figure.png" width="85%" height="75%" />
</div>



--- 

## ChatGLM
<p align="center" width="100%">
<a href="" target="_blank"><img src="assets/chatglm_logo.png" alt="ChatGLM" style="width: 20%; min-width: 20px; display: block; margin: auto;"></a>
</p>

---

## GPT series

<p align="center" width="100%">
<a href="" target="_blank"><img src="assets/chatgpt_logo.png" alt="GPT" style="width: 20%; min-width: 20px; display: block; margin: auto;"></a>
</p>

### Instruct Large Language Models(LLMs) using In-Context Learning

[In-Context Learning](http://arxiv.org/abs/2301.00234) is a method for instructing large language models to improve their performance on specific tasks. It involves iterative learning within specific contexts to fine-tune and train the model, enabling it to better understand and respond to the demands of a particular domain. Through In-Context Learning, we can empower large language models with capabilities such as information extraction, data augmentation, and instruction-driven knowledge graph construction.


#### Information Extraction with LLMs [English](./LLMICL/README.md/#ie-with-large-language-models) | [Chinese](./LLMICL/README_CN.md/#使用大语言模型进行信息抽取)


#### Data Augmentation with LLMs [English](./LLMICL/README.md/#data-augmentation-with-large-language-models) | [Chinese](./LLMICL/README_CN.md/#使用大语言模型进行数据增强)



#### CCKS2023 Instruction-based Knowledge Graph Construction with LLMs [English](./LLMICL/README.md/#ccks2023-instruction-based-knowledge-graph-construction-with-large-language-models) | [Chinese](./LLMICL/README_CN.md/#使用大语言模型完成ccks2023指令驱动的知识图谱构建)



